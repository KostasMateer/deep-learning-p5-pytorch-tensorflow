{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a2cf81-87ed-4352-a4f4-d5959cf05b9f",
   "metadata": {},
   "source": [
    "### Code Review\n",
    "- Basel\n",
    "    - Said code looked great\n",
    "    - He said he doesn't know what else to do in order to increase accuracy\n",
    "    - He said maybe use PyTorch in order to fine tune the model more, as tensorflow is more hands off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec129cbb-0d00-4743-97b5-f8e789894728",
   "metadata": {},
   "source": [
    "# EMNIST BYCLASS WRITEUP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc5191-9c5e-4017-a6b6-7c089814d413",
   "metadata": {},
   "source": [
    "## Before Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867bc088-51c6-467d-9057-a1f6ea0efc48",
   "metadata": {},
   "source": [
    "Had to figure out how to load in the EMNIST ByClass Dataset. Used scipy.io in order to load in .mat files of the dataset. The dataset is split into training and testing images. Had to reshape the arrays because it was flattened in a 697932x784 for the train and a 116323x784 for the test to a 697932x28x28x1 and 116323x28x28x1. We did a one_hot_enocding on the label sets. The EMNIST ByClass set has 62 classes. We also normalized and transposed the image sets because they were rotated by 90 degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81185942-6491-4ee2-8fb1-3291af6a53fc",
   "metadata": {},
   "source": [
    "## Intial Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6fa5e3-4ce4-42e0-8235-dbb329f084d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Architecture\n",
    "- working_model.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c01b1-2e06-403a-9697-c3fa578e2def",
   "metadata": {},
   "source": [
    "    batch_size = 32\n",
    "    num_classes = 62\n",
    "    epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f818cfaa-d3e1-430b-8aa3-88f5a99fe24d",
   "metadata": {
    "tags": []
   },
   "source": [
    "    Model: \"sequential\"\n",
    "    _________________________________________________________________\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    conv2d (Conv2D)              (None, 28, 28, 120)       1200      \n",
    "    _________________________________________________________________\n",
    "    max_pooling2d (MaxPooling2D) (None, 9, 9, 120)         0         \n",
    "    _________________________________________________________________\n",
    "    conv2d_1 (Conv2D)            (None, 9, 9, 120)         129720    \n",
    "    _________________________________________________________________\n",
    "    flatten (Flatten)            (None, 9720)              0         \n",
    "    _________________________________________________________________\n",
    "    dense (Dense)                (None, 1000)              9721000   \n",
    "    _________________________________________________________________\n",
    "    dense_1 (Dense)              (None, 62)                62062     \n",
    "    =================================================================\n",
    "    Total params: 9,913,982\n",
    "    Trainable params: 9,913,982\n",
    "    Non-trainable params: 0\n",
    "    _________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43668f2e-fcf1-4500-b3a6-9f1ef7ef3398",
   "metadata": {},
   "source": [
    "    3636/3636 [==============================] - 7s 2ms/step - loss: 0.8078 - accuracy: 0.7605 - precision: 0.8393 - recall: 0.6863\n",
    "    Test loss: 0.8077670931816101\n",
    "    Test accuracy: 0.7605202794075012\n",
    "    f1_score: 0.7551585"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0930d1a-10c4-48c5-9e49-0252d9d7d638",
   "metadata": {},
   "source": [
    "Not much overfitting ocurring and used the architecture from the other tensorflow model that we built. Cannot change the architecture much because of memory issues, so will try other adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bb07d-418f-45a7-9e9e-22590aa87fad",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18e642-1619-46f0-b0ec-2af28c01b162",
   "metadata": {},
   "source": [
    "After solving the memory issue, going to try to increase the epochs to see if that will increase the model's performance. We raised the epochs to 20 and found an increase of overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd0aaf-f50f-4162-8841-c8e86dfeb74f",
   "metadata": {},
   "source": [
    "### 20 Epochs\n",
    "- working_model2.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246494c-87a3-4e6b-aa7f-8ea69dea5037",
   "metadata": {},
   "source": [
    "After 20 epochs this was our results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e64bc-85ff-4353-940d-316d6019af69",
   "metadata": {},
   "source": [
    "    3636/3636 [==============================] - 9s 2ms/step - loss: 0.5356 - accuracy: 0.8259 - precision: 0.8672 - recall: 0.7870\n",
    "    Test loss: 0.5356273055076599\n",
    "    Test accuracy: 0.8258556127548218"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b1c8e-38fe-4448-97dc-5470817608b1",
   "metadata": {},
   "source": [
    "However we realized that accuracy is not the only metric needed to test whether or not our model is working correctly. The classes are unbalanced and need to check the performance on each class. The report printed by scikit learn's metric module shows the performance for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf66750c-fb62-4399-a5b1-47e6e2a4e7e5",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.68      0.78      0.73      5778\n",
    "           1       0.65      0.94      0.77      6330\n",
    "           2       0.92      0.95      0.93      5869\n",
    "           3       0.96      0.96      0.96      5969\n",
    "           4       0.92      0.96      0.94      5619\n",
    "           5       0.91      0.87      0.89      5190\n",
    "           6       0.95      0.96      0.95      5705\n",
    "           7       0.96      0.98      0.97      6139\n",
    "           8       0.92      0.95      0.93      5633\n",
    "           9       0.91      0.96      0.93      5686\n",
    "          10       0.83      0.89      0.86      1062\n",
    "          11       0.78      0.76      0.77       648\n",
    "          12       0.75      0.84      0.79      1739\n",
    "          13       0.84      0.79      0.81       779\n",
    "          14       0.89      0.79      0.84       851\n",
    "          15       0.76      0.85      0.80      1440\n",
    "          16       0.78      0.76      0.77       447\n",
    "          17       0.83      0.83      0.83       521\n",
    "          18       0.60      0.44      0.51      2048\n",
    "          19       0.84      0.74      0.79       626\n",
    "          20       0.63      0.70      0.66       382\n",
    "          21       0.82      0.93      0.87       810\n",
    "          22       0.73      0.94      0.82      1485\n",
    "          23       0.87      0.91      0.89      1351\n",
    "          24       0.61      0.57      0.59      4156\n",
    "          25       0.81      0.86      0.83      1397\n",
    "          26       0.76      0.79      0.77       413\n",
    "          27       0.87      0.84      0.85       809\n",
    "          28       0.76      0.89      0.82      3508\n",
    "          29       0.90      0.90      0.90      1576\n",
    "          30       0.74      0.93      0.83      2002\n",
    "          31       0.63      0.63      0.63       796\n",
    "          32       0.75      0.82      0.78       806\n",
    "          33       0.74      0.60      0.66       432\n",
    "          34       0.67      0.72      0.69       798\n",
    "          35       0.75      0.47      0.58       464\n",
    "          36       0.80      0.83      0.81      1644\n",
    "          37       0.81      0.77      0.79       853\n",
    "          38       0.25      0.00      0.00       432\n",
    "          39       0.92      0.93      0.93      1683\n",
    "          40       0.89      0.96      0.92      4092\n",
    "          41       0.46      0.13      0.20       400\n",
    "          42       0.54      0.30      0.38       589\n",
    "          43       0.88      0.89      0.88      1479\n",
    "          44       0.71      0.29      0.42       427\n",
    "          45       0.64      0.62      0.63       317\n",
    "          46       0.70      0.49      0.58       466\n",
    "          47       0.48      0.12      0.19      2535\n",
    "          48       0.35      0.02      0.03       464\n",
    "          49       0.91      0.90      0.90      1898\n",
    "          50       0.10      0.00      0.00       466\n",
    "          51       0.43      0.38      0.41       368\n",
    "          52       0.62      0.22      0.32       505\n",
    "          53       0.91      0.95      0.93      2320\n",
    "          54       0.14      0.00      0.01       437\n",
    "          55       0.91      0.92      0.91      2965\n",
    "          56       0.43      0.03      0.06       482\n",
    "          57       0.45      0.43      0.44       468\n",
    "          58       0.68      0.54      0.60       467\n",
    "          59       0.61      0.66      0.63       470\n",
    "          60       0.51      0.12      0.19       381\n",
    "          61       0.56      0.47      0.51       451\n",
    "\n",
    "     accuracy                           0.83    116323\n",
    "    macro avg       0.72      0.67      0.67    116323\n",
    "    weighted avg       0.81      0.83      0.81    116323"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f9235-79b3-4a48-9d1b-eba8a1c95c12",
   "metadata": {},
   "source": [
    "Here it is seen that some classes such as 37, 41, 42, 44, 47, 48, 50, 51, 52, 54, 56, 57, and 60 are performing extremely poorly. Attempted to create a heatmap as a graphic, but the amount of classes make it too difficult, so the report helps with showing performances by class. Going to attempt to train the model by using class weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff32d6-4cf6-4c70-9a7f-108d9e03546a",
   "metadata": {},
   "source": [
    "### Using Class Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94d853-3792-4032-a2c8-7b2dad4ca9cc",
   "metadata": {},
   "source": [
    "The results were lower overall accuracy, but an increase in f1_scores for classes that were scoring poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe601a4-4733-4ec4-9621-d012de42ad58",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Full Report:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3bc587-3361-431e-990e-c610237cd44b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "    \n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "           0       0.74      0.51      0.61      5778\n",
    "           1       0.70      0.75      0.73      6330\n",
    "           2       0.96      0.79      0.87      5869\n",
    "           3       0.96      0.89      0.93      5969\n",
    "           4       0.95      0.85      0.90      5619\n",
    "           5       0.92      0.75      0.83      5190\n",
    "           6       0.97      0.87      0.92      5705\n",
    "           7       0.97      0.92      0.94      6139\n",
    "           8       0.94      0.82      0.88      5633\n",
    "           9       0.91      0.83      0.87      5686\n",
    "          10       0.83      0.86      0.84      1062\n",
    "          11       0.52      0.84      0.64       648\n",
    "          12       0.70      0.70      0.70      1739\n",
    "          13       0.56      0.89      0.69       779\n",
    "          14       0.72      0.84      0.78       851\n",
    "          15       0.76      0.64      0.70      1440\n",
    "          16       0.60      0.82      0.69       447\n",
    "          17       0.78      0.83      0.80       521\n",
    "          18       0.47      0.52      0.49      2048\n",
    "          19       0.69      0.77      0.73       626\n",
    "          20       0.62      0.58      0.60       382\n",
    "          21       0.76      0.94      0.84       810\n",
    "          22       0.80      0.44      0.57      1485\n",
    "          23       0.85      0.87      0.86      1351\n",
    "          24       0.58      0.44      0.50      4156\n",
    "          25       0.78      0.70      0.74      1397\n",
    "          26       0.55      0.87      0.68       413\n",
    "          27       0.80      0.84      0.82       809\n",
    "          28       0.75      0.40      0.52      3508\n",
    "          29       0.88      0.91      0.89      1576\n",
    "          30       0.82      0.54      0.65      2002\n",
    "          31       0.60      0.61      0.60       796\n",
    "          32       0.85      0.61      0.71       806\n",
    "          33       0.72      0.57      0.64       432\n",
    "          34       0.62      0.65      0.64       798\n",
    "          35       0.41      0.73      0.52       464\n",
    "          36       0.71      0.81      0.76      1644\n",
    "          37       0.57      0.88      0.69       853\n",
    "          38       0.24      0.30      0.27       432\n",
    "          39       0.88      0.92      0.90      1683\n",
    "          40       0.93      0.88      0.90      4092\n",
    "          41       0.30      0.49      0.37       400\n",
    "          42       0.29      0.47      0.36       589\n",
    "          43       0.89      0.85      0.87      1479\n",
    "          44       0.30      0.44      0.35       427\n",
    "          45       0.45      0.70      0.55       317\n",
    "          46       0.51      0.67      0.58       466\n",
    "          47       0.40      0.25      0.31      2535\n",
    "          48       0.30      0.74      0.43       464\n",
    "          49       0.92      0.86      0.89      1898\n",
    "          50       0.07      0.38      0.11       466\n",
    "          51       0.30      0.62      0.41       368\n",
    "          52       0.19      0.48      0.27       505\n",
    "          53       0.90      0.93      0.91      2320\n",
    "          54       0.10      0.56      0.17       437\n",
    "          55       0.90      0.85      0.87      2965\n",
    "          56       0.26      0.63      0.37       482\n",
    "          57       0.41      0.47      0.44       468\n",
    "          58       0.49      0.84      0.62       467\n",
    "          59       0.54      0.72      0.61       470\n",
    "          60       0.25      0.47      0.33       381\n",
    "          61       0.32      0.61      0.42       451\n",
    "\n",
    "    accuracy                           0.74    116323\n",
    "    macro avg       0.64      0.70      0.65    116323\n",
    "    weighted avg       0.79      0.74      0.76    116323"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e2101-deb5-476d-be27-49270543dfc4",
   "metadata": {},
   "source": [
    "Notable improvements\n",
    "    \n",
    "    class    precision recall    f1-score   support\n",
    "    37       0.57      0.88      0.69       853\n",
    "    41       0.30      0.49      0.37       400\n",
    "    47       0.40      0.25      0.31      2535\n",
    "    48       0.30      0.74      0.43       464\n",
    "    50       0.07      0.38      0.11       466\n",
    "    54       0.10      0.56      0.17       437\n",
    "    56       0.26      0.63      0.37       482\n",
    "    60       0.25      0.47      0.33       381"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9724e74-cd41-405a-ba45-1410cf7c6229",
   "metadata": {},
   "source": [
    "Going to continue to use class weight in future iterations of the model in order to force the model to take under represented classes into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce1733-7169-4eaf-b7e5-caba2b8ba4f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### New Architecture\n",
    "\n",
    "- working_model3.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d486ca-5a3a-4390-982b-74e07c9ee8e4",
   "metadata": {},
   "source": [
    "    Model: \"sequential_1\"\n",
    "    _________________________________________________________________\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    conv2d_3 (Conv2D)            (None, 28, 28, 120)       1200      \n",
    "    _________________________________________________________________\n",
    "    max_pooling2d_2 (MaxPooling2 (None, 9, 9, 120)         0         \n",
    "    _________________________________________________________________\n",
    "    conv2d_4 (Conv2D)            (None, 9, 9, 120)         129720    \n",
    "    _________________________________________________________________\n",
    "    max_pooling2d_3 (MaxPooling2 (None, 3, 3, 120)         0         \n",
    "    _________________________________________________________________\n",
    "    conv2d_5 (Conv2D)            (None, 3, 3, 120)         129720    \n",
    "    _________________________________________________________________\n",
    "    max_pooling2d_4 (MaxPooling2 (None, 1, 1, 120)         0         \n",
    "    _________________________________________________________________\n",
    "    flatten_1 (Flatten)          (None, 120)               0         \n",
    "    _________________________________________________________________\n",
    "    dense_2 (Dense)              (None, 1000)              121000    \n",
    "    _________________________________________________________________\n",
    "    dense_3 (Dense)              (None, 62)                62062     \n",
    "    =================================================================\n",
    "    Total params: 443,702\n",
    "    Trainable params: 443,702\n",
    "    Non-trainable params: 0\n",
    "    _________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ec8be9-0e41-44d2-a3f1-142ec8f1b04d",
   "metadata": {},
   "source": [
    "Wanted to see what messing around with the architecture did, but it did not do much to the model, but slightly decreased it. The training time for this model with 20 epochs took over an hour, so do not think this architecture was useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56827190-5c24-48cc-b185-1debb42d0f4b",
   "metadata": {},
   "source": [
    "    3636/3636 [==============================] - 7s 2ms/step - loss: 0.9480 - accuracy: 0.6812 - precision: 0.8312 - recall: 0.5153\n",
    "    Test loss: 0.9480129480361938\n",
    "    Test accuracy: 0.6811808347702026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287089b-9c61-40f7-944c-cc129fdc3f8f",
   "metadata": {},
   "source": [
    "Going to go back to the original architecture and try a different optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ccea0a-fa4c-4660-a825-134e8a26da15",
   "metadata": {},
   "source": [
    "### ADAM Optimizer\n",
    "- working_model4.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f9394a-bd28-475d-ac9c-aa8a0cf6eef3",
   "metadata": {},
   "source": [
    "We determined that the Adam optimizer was the best choice for this problem because it produced the most balanced results across all classes compared to the other choices. We were using the AdaDelta optimizer, but that was performing less than the Adam. The results from the report are shown below after 20 epoch.\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.71      0.69      0.70      5778\n",
    "           1       0.79      0.44      0.56      6330\n",
    "           2       0.95      0.91      0.93      5869\n",
    "           3       0.99      0.98      0.98      5969\n",
    "           4       0.97      0.93      0.95      5619\n",
    "           5       0.95      0.88      0.91      5190\n",
    "           6       0.98      0.87      0.93      5705\n",
    "           7       0.98      0.98      0.98      6139\n",
    "           8       0.99      0.89      0.94      5633\n",
    "           9       0.95      0.87      0.91      5686\n",
    "          10       0.90      0.94      0.92      1062\n",
    "          11       0.82      0.90      0.86       648\n",
    "          12       0.77      0.50      0.61      1739\n",
    "          13       0.67      0.91      0.77       779\n",
    "          14       0.90      0.92      0.91       851\n",
    "          15       0.79      0.72      0.76      1440\n",
    "          16       0.58      0.91      0.71       447\n",
    "          17       0.85      0.92      0.88       521\n",
    "          18       0.44      0.60      0.51      2048\n",
    "          19       0.64      0.89      0.74       626\n",
    "          20       0.59      0.73      0.65       382\n",
    "          21       0.80      0.93      0.86       810\n",
    "          22       0.80      0.61      0.70      1485\n",
    "          23       0.92      0.93      0.92      1351\n",
    "          24       0.65      0.46      0.54      4156\n",
    "          25       0.84      0.80      0.82      1397\n",
    "          26       0.81      0.86      0.83       413\n",
    "          27       0.88      0.92      0.90       809\n",
    "          28       0.81      0.64      0.71      3508\n",
    "          29       0.87      0.93      0.90      1576\n",
    "          30       0.81      0.63      0.71      2002\n",
    "          31       0.62      0.64      0.63       796\n",
    "          32       0.73      0.80      0.76       806\n",
    "          33       0.64      0.72      0.68       432\n",
    "          34       0.65      0.70      0.67       798\n",
    "          35       0.53      0.67      0.59       464\n",
    "          36       0.86      0.86      0.86      1644\n",
    "          37       0.65      0.89      0.75       853\n",
    "          38       0.23      0.58      0.33       432\n",
    "          39       0.96      0.96      0.96      1683\n",
    "          40       0.97      0.94      0.96      4092\n",
    "          41       0.32      0.49      0.39       400\n",
    "          42       0.40      0.58      0.48       589\n",
    "          43       0.91      0.95      0.93      1479\n",
    "          44       0.29      0.50      0.36       427\n",
    "          45       0.60      0.66      0.63       317\n",
    "          46       0.65      0.62      0.63       466\n",
    "          47       0.33      0.56      0.42      2535\n",
    "          48       0.32      0.59      0.41       464\n",
    "          49       0.93      0.90      0.91      1898\n",
    "          50       0.07      0.21      0.11       466\n",
    "          51       0.41      0.52      0.46       368\n",
    "          52       0.29      0.52      0.37       505\n",
    "          53       0.94      0.94      0.94      2320\n",
    "          54       0.12      0.39      0.19       437\n",
    "          55       0.93      0.90      0.91      2965\n",
    "          56       0.28      0.51      0.36       482\n",
    "          57       0.43      0.46      0.44       468\n",
    "          58       0.61      0.61      0.61       467\n",
    "          59       0.67      0.60      0.63       470\n",
    "          60       0.36      0.51      0.42       381\n",
    "          61       0.42      0.55      0.47       451\n",
    "\n",
    "    accuracy                           0.79    116323\n",
    "    macro avg       0.69      0.73      0.70    116323\n",
    "    weighted avg       0.83      0.79      0.80    116323\n",
    "    \n",
    "    3636/3636 [==============================] - 9s 2ms/step - loss: 0.7828\n",
    "    accuracy: 0.7907 - precision: 0.8298 - recall: 0.7439\n",
    "    Test loss: 0.7828474044799805\n",
    "    Test accuracy: 0.7907378673553467"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cff352-30db-45b5-b3bc-360bafff3cd8",
   "metadata": {},
   "source": [
    "Training model again for another 20 epochs to see what the result will be. We found that the model's loss started to go into the other direction; meaning that the model most likely began memorizing a few classes, and not classifying the others correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd24dd-55ce-4edd-8a11-8b7249a1ee81",
   "metadata": {},
   "source": [
    "## Complications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a18e2-c96b-4295-a6bb-bd31b23649bd",
   "metadata": {},
   "source": [
    "The most difficult issues that arised from this project stemmed from the dataset itself. The size of it caused our kernel to die multiple times. One way to avoid it was to split the project into two parts. We load in and train the model on the training data, then we save the model. Next part is loading the saved model into a different file and only loading in the test data, then evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d9671-8c07-4548-83d9-17bb464bf5df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow +GPU",
   "language": "python",
   "name": "python3-tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
